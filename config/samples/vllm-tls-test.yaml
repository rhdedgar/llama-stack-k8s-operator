---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-ca-bundle
  namespace: llama-stack-test
data:
  ca-bundle.crt: |
    # This will be populated by the test with the actual CA bundle
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-vllm-config
  namespace: llama-stack-test
data:
  run.yaml: |
    # Llama Stack Configuration for vLLM with TLS
    version: '2'
    image_name: remote-vllm
    apis:
    - inference
    providers:
      inference:
      - provider_id: vllm
        provider_type: "remote::vllm"
        config:
          url: "https://vllm-server.vllm-dist.svc.cluster.local:8000/v1"
    models:
      - model_id: "meta-llama/Llama-3.2-1B-Instruct"
        provider_id: vllm
        model_type: llm
    server:
      port: 8321
---
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: vllm-tls-test
  namespace: llama-stack-test
spec:
  replicas: 1
  server:
    distribution:
      name: remote-vllm
    containerSpec:
      port: 8321
      env:
      - name: INFERENCE_MODEL
        value: "meta-llama/Llama-3.2-1B-Instruct"
      - name: VLLM_URL
        value: "https://vllm-server.vllm-dist.svc.cluster.local:8000/v1"
    userConfig:
      configMapName: llama-stack-vllm-config
    tlsConfig:
      caBundle:
        configMapName: vllm-ca-bundle
        key: ca-bundle.crt
