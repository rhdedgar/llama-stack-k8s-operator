apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-with-ca-bundle
spec:
  replicas: 1
  server:
    distribution:
      name: remote-vllm
    containerSpec:
      port: 8321
      env:
      - name: INFERENCE_MODEL
        value: "meta-llama/Llama-3.2-1B-Instruct"
      - name: VLLM_URL
        value: "https://vllm-server.vllm-dist.svc.cluster.local:8000/v1"
      - name: VLLM_TLS_VERIFY
        value: "/etc/ssl/certs/ca-bundle.crt"
    userConfig:
      customConfig: |
        # Llama Stack Configuration
        version: '2'
        image_name: remote-vllm
        apis:
        - inference
        providers:
          inference:
          - provider_id: vllm
            provider_type: "remote::vllm"
            config:
              url: "https://vllm-server.vllm-dist.svc.cluster.local:8000/v1"
        models:
          - model_id: "meta-llama/Llama-3.2-1B-Instruct"
            provider_id: vllm
            model_type: llm
        server:
          port: 8321
    tlsConfig:
      # caBundle must contain valid PEM formatted data
      caBundle: "PLACEHOLDER_CA_BUNDLE"
