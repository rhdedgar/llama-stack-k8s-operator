# Implementation Plan: Deploy-Time Modularity - Level 1

**Branch**: `001-deploy-time-providers-l1`
**Date**: 2025-11-12
**Spec**: `specs/001-deploy-time-providers-l1/spec.md`

## Summary

Implement external provider injection for llama-stack K8s operator using init containers to install Python provider packages at pod startup. This enables users to add custom/third-party providers without rebuilding distribution images.

**Technical Approach**: Two-phase init container architecture using `extra-providers.yaml` schema:
1. **Phase 1 - Install**: N init containers (one per provider, in CRD order) install Python packages and copy metadata
2. **Phase 2 - Merge**: Single init container (operator image) generates `extra-providers.yaml` from metadata, merges with user run.yaml (if exists)
3. **Main container**: Uses merged run.yaml, PYTHONPATH includes external provider packages

**Forward Compatibility**: The `extra-providers.yaml` schema enables future migration to native LlamaStack support (Phase 2) with minimal operator changes.

**Note on Phase Terminology**: This plan describes *implementation phases* (development sequence: Phase 0-6). The spec's "Container Startup Sequence" section describes *runtime phases* (pod lifecycle: Phase 1-5). These are complementary views:
- Plan Phases 2-3 (Init Container Generation, run.yaml Merging) implement spec's runtime Phases 1-3 (Provider Install, Config Extract, Merge)
- Plan Phase 4 (Controller Integration) orchestrates all runtime phases
- Spec runtime phases describe what happens at pod startup; plan phases describe when to build each component

## Technical Context

**Language/Version**: Go 1.21+ (operator), Python 3.11+ (providers)
**Primary Dependencies**: controller-runtime, client-go, pip (for installation)
**Storage**: emptyDir volume for sharing between init/main containers
**Testing**: Go test + integration tests using envtest
**Target Platform**: Kubernetes 1.21+
**Performance Goals**: Init container execution time (excluding image pull) < 30s per provider for typical provider with 5-10 dependencies
**Constraints**: No network access during provider installation, deterministic ordering
**Scale/Scope**: Support 1-10 external providers per LLSD instance

## Constitution Check

*GATE: Must pass before implementation.*

**Checks (based on K8s operator best practices)**:
- ✅ CRD changes follow Kubernetes API conventions
- ✅ Status reporting includes meaningful conditions
- ✅ Error messages are actionable for end users
- ✅ Reconciliation is idempotent
- ✅ No cluster-wide permissions required (namespace-scoped)

## Project Structure

### Documentation (this feature)

```
specs/001-deploy-time-providers-l1/
├── spec.md                  # Requirements and contracts
├── plan.md                  # This file - implementation approach
├── implementation-notes.md  # Technical brainstorming insights
└── tasks.md                 # Generated by /speckit.tasks command
```

### Source Code

```
api/v1alpha1/
├── llamastackdistribution_types.go  # Add ExternalProvidersSpec, ExternalProviderRef

controllers/
├── llamastackdistribution_controller.go  # Add external provider logic
├── external_providers.go                 # New: external provider handling
└── validation.go                         # New: provider validation

pkg/
├── provider/
│   ├── metadata.go          # Provider metadata parsing
│   └── validation.go        # Provider validation logic
└── deploy/
    ├── initcontainer.go     # Init container generation
    └── runyaml.go           # run.yaml merging logic

tests/
├── integration/
│   └── external_providers_test.go  # Integration tests
└── unit/
    ├── metadata_test.go
    ├── validation_test.go
    └── merge_test.go
```

**Structure Decision**: Keep external provider logic in separate files from main controller to maintain separation of concerns. Provider-specific functionality in `pkg/provider/` for reusability.

## Implementation Phases

### Phase 0: CRD Updates

**Objective**: Add external provider fields to LlamaStackDistribution CRD

**Tasks**:
1. Add `ExternalProvidersSpec` struct to `llamastackdistribution_types.go`
2. Add `ExternalProviderRef` struct with validation tags
3. Add `ExternalProviderStatus` struct to status
4. Generate CRD manifests: `make manifests`
5. Update examples with external provider usage

**Files Modified**:
- `api/v1alpha1/llamastackdistribution_types.go`

**Testing**:
- Unit test CRD validation (duplicate provider IDs, invalid names)
- Integration test CRD creation/update

**Example Code**:
```go
// ExternalProvidersSpec organizes external providers by API type
type ExternalProvidersSpec struct {
	Inference    []ExternalProviderRef `json:"inference,omitempty"`
	Safety       []ExternalProviderRef `json:"safety,omitempty"`
	Agents       []ExternalProviderRef `json:"agents,omitempty"`
	VectorIO     []ExternalProviderRef `json:"vectorIo,omitempty"`
	DatasetIO    []ExternalProviderRef `json:"datasetIo,omitempty"`
	Scoring      []ExternalProviderRef `json:"scoring,omitempty"`
	Eval         []ExternalProviderRef `json:"eval,omitempty"`
	ToolRuntime  []ExternalProviderRef `json:"toolRuntime,omitempty"`
	PostTraining []ExternalProviderRef `json:"postTraining,omitempty"`
}

// ExternalProviderRef references an external provider container image
type ExternalProviderRef struct {
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:Pattern=`^[a-z0-9]([-a-z0-9]*[a-z0-9])?$`
	ProviderID string `json:"providerId"`

	// +kubebuilder:validation:Required
	Image string `json:"image"`

	// +kubebuilder:default=IfNotPresent
	// +kubebuilder:validation:Enum=Always;Never;IfNotPresent
	ImagePullPolicy corev1.PullPolicy `json:"imagePullPolicy,omitempty"`

	Config *apiextensionsv1.JSON `json:"config,omitempty"`
}
```

### Phase 1: Provider Metadata Handling

**Objective**: Implement provider metadata parsing and validation

**Tasks**:
1. Create `pkg/provider/metadata.go` with ProviderMetadata struct
2. Implement YAML parsing with validation
3. Create `pkg/provider/validation.go` for metadata validation
4. Write unit tests for all validation rules

**Note**: Init containers will be generated in CRD order (not alphabetical) to preserve user intent.

**Files Created**:
- `pkg/provider/metadata.go`
- `pkg/provider/validation.go`
- `tests/unit/metadata_test.go`

**Implementation**:

```go
// pkg/provider/metadata.go
package provider

import (
	"fmt"
	"os"
	"path/filepath"
	"gopkg.in/yaml.v3"
)

// ProviderMetadata represents the lls-provider-spec.yaml structure
type ProviderMetadata struct {
	APIVersion string `yaml:"apiVersion"`
	Kind       string `yaml:"kind"`
	Metadata   struct {
		Name        string `yaml:"name"`
		Version     string `yaml:"version"`
		Vendor      string `yaml:"vendor"`
		Description string `yaml:"description,omitempty"`
		Maintainer  string `yaml:"maintainer,omitempty"`
	} `yaml:"metadata"`
	Spec struct {
		PackageName      string   `yaml:"packageName"`
		ProviderType     string   `yaml:"providerType"`
		API              string   `yaml:"api"`
		WheelPath        string   `yaml:"wheelPath"`
		DependencyWheels []string `yaml:"dependencyWheels,omitempty"`
	} `yaml:"spec"`
}

// LoadProviderMetadata reads and parses provider metadata from file
func LoadProviderMetadata(path string) (*ProviderMetadata, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("failed to read metadata file: %w", err)
	}

	var metadata ProviderMetadata
	if err := yaml.Unmarshal(data, &metadata); err != nil {
		return nil, fmt.Errorf("failed to parse metadata YAML: %w", err)
	}

	if err := ValidateMetadata(&metadata); err != nil {
		return nil, fmt.Errorf("metadata validation failed: %w", err)
	}

	return &metadata, nil
}

// ValidateMetadata validates provider metadata structure and values
func ValidateMetadata(m *ProviderMetadata) error {
	if m.APIVersion != "llamastack.io/v1alpha1" {
		return fmt.Errorf("invalid apiVersion: expected llamastack.io/v1alpha1, got %s", m.APIVersion)
	}

	if m.Kind != "ProviderPackage" {
		return fmt.Errorf("invalid kind: expected ProviderPackage, got %s", m.Kind)
	}

	if m.Spec.PackageName == "" {
		return fmt.Errorf("spec.packageName is required")
	}
	// PackageName will be used by LlamaStack to import the provider module:
	// module = importlib.import_module(provider_spec.module)
	// This maps to the 'module:' field in run.yaml/extra-providers.yaml

	if m.Spec.ProviderType == "" {
		return fmt.Errorf("spec.providerType is required")
	}

	if m.Spec.API == "" {
		return fmt.Errorf("spec.api is required")
	}

	// Validate API is known type
	validAPIs := map[string]bool{
		"inference": true, "safety": true, "agents": true,
		"vector_io": true, "datasetio": true, "scoring": true,
		"eval": true, "tool_runtime": true, "post_training": true,
	}

	if !validAPIs[m.Spec.API] {
		return fmt.Errorf("invalid spec.api: %s", m.Spec.API)
	}

	return nil
}
```

### Phase 2: Init Container Generation

**Objective**: Generate init containers for external provider installation (Phase 1) and merge init container (Phase 2)

**Tasks**:
1. Create `pkg/deploy/initcontainer.go`
2. Implement Phase 1 init container template generation (provider install)
3. Implement Phase 2 merge init container (operator image with merge tool)
4. Add shared volume configuration
5. Implement CRD ordering for Phase 1 init containers
6. Write unit tests for init container generation

**Phase 1 Init Containers**: Install provider packages and copy metadata (one per provider, in CRD order)
**Phase 2 Merge Init Container**: Runs merge tool from operator image to generate `extra-providers.yaml` and merge with user run.yaml

**Files Created**:
- `pkg/deploy/initcontainer.go`
- `tests/unit/initcontainer_test.go`

**Implementation**:

```go
// pkg/deploy/initcontainer.go
package deploy

import (
	"fmt"
	"sort"
	corev1 "k8s.io/api/core/v1"
	llamav1alpha1 "github.com/llamastack/llama-stack-k8s-operator/api/v1alpha1"
)

// GenerateInitContainers creates init containers for external providers
// Returns containers in CRD order (order they appear in the spec) for determinism
func GenerateInitContainers(instance *llamav1alpha1.LlamaStackDistribution) []corev1.Container {
	if instance.Spec.Server.ExternalProviders == nil {
		return nil
	}

	// Collect all providers from all API types IN CRD ORDER
	allProviders := collectAllProviders(instance)

	// NO SORTING - preserve CRD order for determinism
	// Generate init containers
	containers := make([]corev1.Container, 0, len(allProviders))
	for _, p := range allProviders {
		containers = append(containers, generateProviderInitContainer(p.ref, p.api))
	}

	return containers
}

type providerWithAPI struct {
	ref llamav1alpha1.ExternalProviderRef
	api string
}

func collectAllProviders(instance *llamav1alpha1.LlamaStackDistribution) []providerWithAPI {
	providers := []providerWithAPI{}

	if instance.Spec.Server.ExternalProviders.Inference != nil {
		for _, p := range instance.Spec.Server.ExternalProviders.Inference {
			providers = append(providers, providerWithAPI{ref: p, api: "inference"})
		}
	}

	if instance.Spec.Server.ExternalProviders.Safety != nil {
		for _, p := range instance.Spec.Server.ExternalProviders.Safety {
			providers = append(providers, providerWithAPI{ref: p, api: "safety"})
		}
	}

	// ... repeat for all API types ...

	return providers
}

func generateProviderInitContainer(ref llamav1alpha1.ExternalProviderRef, apiType string) corev1.Container {
	initScript := fmt.Sprintf(`
set -e
echo "Installing external provider: %s"

# Validate metadata file exists
if [ ! -f /lls-provider/lls-provider-spec.yaml ]; then
  echo "ERROR: Missing /lls-provider/lls-provider-spec.yaml in image %s"
  exit 1
fi

# Validate package directory exists
if [ ! -d /lls-provider/packages ]; then
  echo "ERROR: Missing /lls-provider/packages/ directory in image %s"
  exit 1
fi

# Install all wheels to shared location
echo "Installing provider packages..."
pip install /lls-provider/packages/*.whl \
  --target /shared/python-packages \
  --no-index \
  --find-links /lls-provider/packages \
  --no-cache-dir \
  --disable-pip-version-check

# Copy metadata file for validation
mkdir -p /shared/metadata
cp /lls-provider/lls-provider-spec.yaml /shared/metadata/%s.yaml

echo "Successfully installed provider: %s"
`, ref.ProviderID, ref.Image, ref.Image, ref.ProviderID, ref.ProviderID)

	pullPolicy := ref.ImagePullPolicy
	if pullPolicy == "" {
		pullPolicy = corev1.PullIfNotPresent
	}

	return corev1.Container{
		Name:            fmt.Sprintf("external-provider-%s", ref.ProviderID),
		Image:           ref.Image,
		ImagePullPolicy: pullPolicy,
		Command:         []string{"/bin/sh", "-c", initScript},
		VolumeMounts: []corev1.VolumeMount{
			{
				Name:      "external-providers",
				MountPath: "/shared",
			},
		},
	}
}

// AddExternalProvidersVolume adds the shared volume for external providers
func AddExternalProvidersVolume(podSpec *corev1.PodSpec) {
	podSpec.Volumes = append(podSpec.Volumes, corev1.Volume{
		Name: "external-providers",
		VolumeSource: corev1.VolumeSource{
			EmptyDir: &corev1.EmptyDirVolumeSource{},
		},
	})
}

// MountExternalProvidersVolume adds volume mount to main container
func MountExternalProvidersVolume(container *corev1.Container) {
	container.VolumeMounts = append(container.VolumeMounts, corev1.VolumeMount{
		Name:      "external-providers",
		MountPath: "/opt/llama-stack/external-providers",
		ReadOnly:  true,
	})
}

// UpdatePythonPath prepends external providers to PYTHONPATH
func UpdatePythonPath(container *corev1.Container) {
	pythonPath := "/opt/llama-stack/external-providers/python-packages"

	// Find existing PYTHONPATH or create new
	found := false
	for i := range container.Env {
		if container.Env[i].Name == "PYTHONPATH" {
			container.Env[i].Value = pythonPath + ":" + container.Env[i].Value
			found = true
			break
		}
	}

	if !found {
		container.Env = append(container.Env, corev1.EnvVar{
			Name:  "PYTHONPATH",
			Value: pythonPath,
		})
	}
}
```

**Build System Changes**:

The operator Dockerfile must be updated to build and include the merge-run-yaml binary:

```dockerfile
# Build stage - add merge tool
FROM golang:1.21 as builder
WORKDIR /workspace

# ... existing COPY and build steps ...

# Build manager binary
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager cmd/main.go

# Build merge-run-yaml binary (NEW)
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o merge-run-yaml cmd/merge-run-yaml/main.go

# Runtime stage
FROM gcr.io/distroless/static:nonroot
WORKDIR /
COPY --from=builder /workspace/manager .
COPY --from=builder /workspace/merge-run-yaml .  # NEW: Include merge tool

ENTRYPOINT ["/manager"]
```

**Task Reference**: This change is captured in T024g "Update Dockerfile to include merge-run-yaml binary"

### Phase 3: run.yaml Merging Logic

**Objective**: Implement merge tool that generates `extra-providers.yaml` and merges with user run.yaml

**Tasks**:
1. Create `cmd/merge-run-yaml/main.go` - standalone binary for merge init container
2. Create `pkg/deploy/runyaml.go` - core merge logic
3. Implement extra-providers.yaml generation from provider metadata
4. Implement merge algorithm (user run.yaml + extra-providers.yaml → final run.yaml)
5. Add conflict detection and resolution
6. Add API placement validation
7. Write comprehensive unit tests for all merge scenarios

**Note**: This merge tool will be packaged in the operator image and run in Phase 2 init container. The `extra-providers.yaml` format matches run.yaml provider structure, enabling future migration to native LlamaStack `--extra-providers` flag support.

**Files Created**:
- `pkg/deploy/runyaml.go`
- `tests/unit/merge_test.go`

**Implementation**:

```go
// pkg/deploy/runyaml.go
package deploy

import (
	"fmt"
	"gopkg.in/yaml.v3"
	llamav1alpha1 "github.com/llamastack/llama-stack-k8s-operator/api/v1alpha1"
	"github.com/llamastack/llama-stack-k8s-operator/pkg/provider"
)

// RunYamlConfig represents the run.yaml structure
type RunYamlConfig struct {
	Version   int                              `yaml:"version"`
	ImageName string                           `yaml:"image_name"`
	APIs      []string                         `yaml:"apis"`
	Providers map[string][]ProviderConfigEntry `yaml:"providers"`
	// ... other fields ...
}

type ProviderConfigEntry struct {
	ProviderID   string                 `yaml:"provider_id"`
	ProviderType string                 `yaml:"provider_type"`
	Module       string                 `yaml:"module,omitempty"`
	Config       map[string]interface{} `yaml:"config"`
}

// MergeRunYaml generates final run.yaml by merging all sources
func MergeRunYaml(
	baseConfig *RunYamlConfig,
	userConfig *RunYamlConfig,
	instance *llamav1alpha1.LlamaStackDistribution,
	metadataDir string,
) (*RunYamlConfig, []string, error) {
	// Start with base or user config
	finalConfig := baseConfig
	if userConfig != nil {
		finalConfig = userConfig // User config completely replaces base
	}

	if finalConfig == nil {
		finalConfig = &RunYamlConfig{
			Version:   2,
			Providers: make(map[string][]ProviderConfigEntry),
		}
	}

	// Validate no duplicate external provider IDs
	if err := validateNoDuplicateExternalProviderIDs(instance); err != nil {
		return nil, nil, err
	}

	// Merge external providers
	warnings := []string{}

	if instance.Spec.Server.ExternalProviders != nil {
		warnings, err := mergeExternalProviders(finalConfig, instance, metadataDir)
		if err != nil {
			return nil, nil, err
		}
	}

	return finalConfig, warnings, nil
}

func mergeExternalProviders(
	config *RunYamlConfig,
	instance *llamav1alpha1.LlamaStackDistribution,
	metadataDir string,
) ([]string, error) {
	warnings := []string{}

	// Process each API type
	apiProviders := map[string][]llamav1alpha1.ExternalProviderRef{
		"inference":    instance.Spec.Server.ExternalProviders.Inference,
		"safety":       instance.Spec.Server.ExternalProviders.Safety,
		"agents":       instance.Spec.Server.ExternalProviders.Agents,
		"vector_io":    instance.Spec.Server.ExternalProviders.VectorIO,
		"datasetio":    instance.Spec.Server.ExternalProviders.DatasetIO,
		"scoring":      instance.Spec.Server.ExternalProviders.Scoring,
		"eval":         instance.Spec.Server.ExternalProviders.Eval,
		"tool_runtime": instance.Spec.Server.ExternalProviders.ToolRuntime,
		"post_training": instance.Spec.Server.ExternalProviders.PostTraining,
	}

	for apiName, providers := range apiProviders {
		if providers == nil {
			continue
		}

		for _, extProvider := range providers {
			// Read metadata
			metadataPath := fmt.Sprintf("%s/%s.yaml", metadataDir, extProvider.ProviderID)
			metadata, err := provider.LoadProviderMetadata(metadataPath)
			if err != nil {
				return nil, fmt.Errorf("failed to load metadata for provider %s: %w",
					extProvider.ProviderID, err)
			}

			// Validate API placement
			if metadata.Spec.API != apiName {
				return nil, &APIPlacementError{
					ProviderID:  extProvider.ProviderID,
					Image:       extProvider.Image,
					DeclaredAPI: metadata.Spec.API,
					PlacedInAPI: apiName,
				}
			}

			// Create provider config entry
			// Note: Module field comes from metadata.Spec.PackageName
			// This becomes the 'module:' field in extra-providers.yaml that
			// LlamaStack will use to import the provider via importlib.import_module()
			entry := ProviderConfigEntry{
				ProviderID:   extProvider.ProviderID,
				ProviderType: metadata.Spec.ProviderType,
				Module:       metadata.Spec.PackageName,
				Config:       convertJSONToMap(extProvider.Config),
			}

			// Check for existing provider with same ID
			existingIndex := findProviderIndexByID(config.Providers[apiName], extProvider.ProviderID)
			if existingIndex >= 0 {
				// Override existing provider with warning
				warnings = append(warnings, fmt.Sprintf(
					"Provider ID '%s' defined in both base config and externalProviders. "+
					"Using external provider (image: %s, type: %s)",
					extProvider.ProviderID, extProvider.Image, metadata.Spec.ProviderType))

				config.Providers[apiName][existingIndex] = entry
			} else {
				// Append new provider
				config.Providers[apiName] = append(config.Providers[apiName], entry)
			}
		}
	}

	return warnings, nil
}

func validateNoDuplicateExternalProviderIDs(instance *llamav1alpha1.LlamaStackDistribution) error {
	seen := make(map[string]string) // provider ID -> API type

	allProviders := collectAllProvidersWithAPI(instance)

	for _, p := range allProviders {
		if existingAPI, exists := seen[p.ref.ProviderID]; exists {
			return fmt.Errorf(
				"duplicate provider_id '%s' in externalProviders: "+
				"found in both %s and %s API sections",
				p.ref.ProviderID, existingAPI, p.api)
		}
		seen[p.ref.ProviderID] = p.api
	}

	return nil
}

type APIPlacementError struct {
	ProviderID  string
	Image       string
	DeclaredAPI string
	PlacedInAPI string
}

func (e *APIPlacementError) Error() string {
	return fmt.Sprintf(
		"Provider '%s' (image: %s) declares api=%s in lls-provider-spec.yaml "+
		"but is placed under externalProviders.%s. "+
		"Move the provider to the correct API section.",
		e.ProviderID, e.Image, e.DeclaredAPI, e.PlacedInAPI)
}
```

### Phase 4: Controller Integration

**Objective**: Integrate external provider logic into main controller reconciliation loop

**Tasks**:
1. Update `buildManifestContext()` to include init containers
2. Modify deployment generation to add volumes and mounts
3. Add status tracking for external providers
4. Implement error handling and status updates
5. Add logging for warnings (metadata mismatches, provider overrides)

**Files Modified**:
- `controllers/llamastackdistribution_controller.go`

**Files Created**:
- `controllers/external_providers.go`

**Implementation Approach**:
1. In `buildManifestContext()`, call `GenerateInitContainers()`
2. Add init containers to pod template spec
3. Add shared volume and mounts
4. Update PYTHONPATH in main container
5. Monitor init container status in `updateStatus()`
6. Report failures in LLSD status

### Phase 5: Status Tracking

**Objective**: Track and report external provider installation status

**Tasks**:
1. Create `updateExternalProviderStatus()` function
2. Monitor init container status
3. Map init container failures to provider status
4. Update LLSD conditions based on provider status
5. Write unit tests for status updates

**Implementation**:

```go
// controllers/external_providers.go
func (r *LlamaStackDistributionReconciler) updateExternalProviderStatus(
	ctx context.Context,
	instance *llamav1alpha1.LlamaStackDistribution,
) error {
	// Get deployment pod
	pod, err := r.getCurrentPod(ctx, instance)
	if err != nil || pod == nil {
		// No pod yet or error - clear status
		instance.Status.ExternalProviderStatus = nil
		return err
	}

	// Track status for each external provider
	providerStatuses := []llamav1alpha1.ExternalProviderStatus{}

	allProviders := collectAllProvidersFromInstance(instance)
	for _, provider := range allProviders {
		initContainerName := fmt.Sprintf("external-provider-%s", provider.ProviderID)

		status := findInitContainerStatus(pod, initContainerName)

		providerStatus := llamav1alpha1.ExternalProviderStatus{
			ProviderID:         provider.ProviderID,
			Image:              provider.Image,
			InitContainerName:  initContainerName,
			LastTransitionTime: metav1.Now(),
		}

		if status == nil {
			providerStatus.Phase = "Pending"
			providerStatus.Message = "Waiting for init container to start"
		} else if status.State.Terminated != nil {
			if status.State.Terminated.ExitCode == 0 {
				providerStatus.Phase = "Ready"
				providerStatus.Message = "Provider installed successfully"
			} else {
				providerStatus.Phase = "Failed"
				providerStatus.Message = extractErrorMessage(status)
			}
		} else if status.State.Running != nil {
			providerStatus.Phase = "Installing"
			providerStatus.Message = "Installing provider packages"
		} else if status.State.Waiting != nil {
			providerStatus.Phase = "Pending"
			providerStatus.Message = status.State.Waiting.Reason
		}

		providerStatuses = append(providerStatuses, providerStatus)
	}

	instance.Status.ExternalProviderStatus = providerStatuses
	return nil
}

func extractErrorMessage(status *corev1.ContainerStatus) string {
	if status.State.Terminated.Message != "" {
		return fmt.Sprintf("Init container failed with exit code %d: %s",
			status.State.Terminated.ExitCode,
			status.State.Terminated.Message)
	}

	return fmt.Sprintf("Init container failed with exit code %d",
		status.State.Terminated.ExitCode)
}
```

### Phase 6: Testing

**Objective**: Comprehensive testing of external provider functionality

**Tasks**:
1. Unit tests for all new packages
2. Integration tests using envtest
3. E2E tests with real provider images
4. Error scenario testing (all error paths)
5. Performance testing (init container startup time)

**Test Cases**:

**Unit Tests**:
- Metadata parsing (valid, invalid, missing fields)
- Init container generation (ordering, script content)
- run.yaml merging (all precedence scenarios)
- Validation logic (duplicate IDs, API mismatches)

**Integration Tests**:
- Single external provider installation
- Multiple providers (same API)
- Multiple providers (different APIs)
- Provider override scenarios
- Dependency conflict detection
- API placement validation

**E2E Tests**:
- Deploy LLSD with external provider
- Verify provider in /v1/providers endpoint
- Call provider through llama-stack API
- Update provider image (rolling update)
- Delete LLSD (cleanup verification)

## Complexity Tracking

> This section filled because we're adding significant new functionality

| Decision | Why Needed | Simpler Alternative Rejected Because |
|----------|------------|-------------------------------------|
| Init containers vs pre-built images | Dynamic provider addition without image rebuilds | Pre-built images require rebuild for every provider combination |
| CRD ordering of init containers | Deterministic, predictable behavior that preserves user intent | Random ordering could cause non-deterministic failures, alphabetical ignores user intent |
| Metadata in image vs CRD | Single source of truth, avoid duplication/mismatch | CRD metadata could diverge from actual provider implementation |
| EmptyDir volume vs ConfigMap | Supports large provider packages, writable | ConfigMap has size limits, read-only |
| extra-providers.yaml schema | Forward-compatible with future LlamaStack native support | Extracting run.yaml from distribution images is brittle and fragile |

## Alternative Approaches Considered

### 1. Sidecar Containers (Level 2a)
**Pros**: Better isolation, independent scaling, separate resource limits
**Cons**: More complex networking, higher resource overhead, harder debugging
**Decision**: Defer to Level 2, use init containers for Level 1 simplicity

### 2. Pre-build Provider Images
**Pros**: Faster startup (no installation), smaller attack surface
**Cons**: Requires rebuild for every combination, loses deploy-time flexibility
**Decision**: Rejected - contradicts goal of deploy-time modularity

### 3. DaemonSet for Provider Installation
**Pros**: Install once per node, reuse across pods
**Cons**: Node-level coupling, complex cleanup, overkill for namespace-scoped resources
**Decision**: Rejected - adds unnecessary complexity

## Extra Providers Schema Design

### Decision: extra-providers.yaml as Forward-Compatible Format

**Rationale**:
- Distribution images contain built-in run.yaml at non-standardized paths
- Extracting run.yaml is brittle and fragile across versions
- Schema evolution in run.yaml would break merge logic across LlamaStack versions
- Better long-term solution: Let LlamaStack handle merge via `--extra-providers` flag

**Current Implementation** (Phase 1 - this feature):
```
Merge Init Container:
  1. Read provider metadata files
  2. Generate extra-providers.yaml (matches run.yaml provider structure)
  3. Merge user run.yaml (if exists) + extra-providers.yaml
  4. Write final run.yaml for main container
```

**Future Enhancement** (Phase 2 - when LlamaStack adds support):
```
Main Container Args:
  llama stack run /etc/llama-stack/run.yaml \
    --extra-providers /etc/extra-providers/extra-providers.yaml
```

**Benefits**:
- ✅ No brittle path discovery for run.yaml in distribution images
- ✅ Schema evolution handled by LlamaStack (not operator)
- ✅ Clean migration path (~20 lines of operator code)
- ✅ Enables other tools to use same schema (Docker Compose, Helm, etc.)

**GitHub Issue**: To be filed with LlamaStack project proposing `--extra-providers` flag support.

## Dependencies and Prerequisites

**Before Implementation**:
- [ ] llama-stack supports module-based provider loading (verify in codebase)
- [ ] Provider images available for testing (create sample provider)
- [ ] Understanding of llama-stack run.yaml structure (documented)
- [ ] Define extra-providers.yaml schema (see spec.md)
- [ ] Create merge tool binary in operator image (cmd/merge-run-yaml)

**External Dependencies**:
- Kubernetes 1.21+ (init container support)
- Python 3.11+ in provider images (for pip install)
- pip available in provider images

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Dependency conflicts not detected until runtime | High - pods crash loop | Add preflight validation (separate spec) |
| Large provider images slow init | Medium - slow startup | Document size limits, use wheel bundles |
| Metadata mismatch with runtime | Low - confusing errors | Log warnings, use runtime values |
| Provider breaks on architecture mismatch | High - silent failures | Add architecture validation in preflight |

## Rollout Plan

**Phase 1: Alpha**
- Feature flag controlled (off by default)
- Documentation: "Experimental"
- Limited to 1-2 providers

**Phase 2: Beta**
- Default enabled
- Full documentation and examples
- Support 5+ providers

**Phase 3: GA**
- Battle-tested in production
- Performance benchmarks published
- Comprehensive troubleshooting guide

## Success Metrics

- Init container startup time < 30s per provider
- Zero false-positive conflict errors
- 90%+ of external provider errors include actionable resolution
- Documentation covers 95% of user questions

## Open Implementation Questions

- [ ] Should we cache provider metadata in operator memory or re-read from volume each reconciliation?
- [ ] How to handle provider image pull failures? Retry policy?
- [ ] Should we support initContainer resource limits per provider?
- [ ] How to test provider functionality end-to-end without running llama-stack server?

## Documentation Requirements

**User Documentation**:
1. Provider image creation guide (Containerfile, metadata format, wheel packaging)
2. CRD reference (all fields with examples)
3. Troubleshooting guide (common errors and fixes)
4. Migration guide (custom images → external providers)

**Developer Documentation**:
1. Provider development guide (implementing get_provider_spec())
2. Operator architecture diagram
3. Testing guide (unit, integration, E2E)
